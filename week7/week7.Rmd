---
title: "Week 7 HW"
author: "Tyler Jackson"
date: "5/13/2019"
output:
  html_document:
    theme: journal
    toc: true
    toc_float: true
    toc_depth: 3
---

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libs
```{r}
library(ISLR)
library(raster)
library(rgdal)
library(dismo)
library(glmnet)
library(fields)
library(mgcv)
```

## Path & Data
```{r}
#set data dir path
dat.path <- "/Users/tyler/Dropbox/_200CN/labs/week7/data"

#unzip week7 data into the data dir (for wednesday q's)
unzip(file.path(dat.path, "week7.zip"), overwrite = TRUE, exdir = dat.path)
```

# Monday
In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.
 


## Question 1

### (a) Fit Model

__Fit a logistic regression model that uses income and balance to predict default.__

```{r}

#bring data into env, not neccesary
df <- Default

#fit a logit model, family = "binomial". see ? family
fit.df <- glm(default ~ income + balance, data = df, family = "binomial")

#check it out
summary(fit.df)
```


### (b) Validation

__Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps__

#### i. Split the sample set into a training set and a validation set.

```{r}
#set seed so we get consistent reproducability
set.seed(513)

#training data numbers for the lm
train <- sample (nrow(df), nrow(df)/2)
```

#### ii. Fit a multiple logistic regression model using only the training observations.
  
```{r}

#run a logit model, using the training subset
fit.train <- glm(default ~ income + balance, data = df, family = "binomial", subset = train)


summary(fit.train)
```
  
#### iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.

```{r}
#get the post probability using the training model on the validation data
df.pred <- predict(fit.train, df[-train,], type = "response")

#code them Yes / No
default <- ifelse(df.pred > 0.5, "Yes", "No")

table(default)
```

#### iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.

```{r}
#Compute VSE, the rate of coded predictions that don't match the actual validation data
mean(default != df[-train,"default"])
```

> There is a `r (mean(default != df[-train,"default"]))*100`% error rate using the validation set method.

### (c) Repeat

__Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.__

```{r}
#` function that repeats the above process
defaultError <- function(formula=default~income+balance,n=10000,s=5000,seed){
  set.seed(seed) #this is not needed but it is also OK
  train <- sample(n,s) #get training set row numbers                       
  fit <- glm(formula, family = 'binomial', data=df,subset=train) #fit model
  p <- predict(fit, df[-train,],type = 'response') #get predictions
  d <- ifelse(p > 0.5, "Yes", "No") #code them y/n
  mean(d != df[-train,"default"]) #get the error (VSE)              
  }
```

```{r}
#for loop does this three times
for (i in 1:3){
  print(defaultError(seed = i))
}
```

> Here we can see that the error rate floats around 2.5% in the repetated validation set methods with different test / training data, with a max of 2.64% and a min of 2.38%. 

### (d) Include Student 

__Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.__

```{r}
#modify the formula to include student
for (i in 4:6){
  print(defaultError(formula = default~income+balance+student, seed=i))
}
```

> The inclusion of student in our model does not seem to affect the error rate in a significant way. It certainly does not lead to a reduction in test error rate.

****
# Wednesday

Load data
```{r}
#read temperature data
d <- read.csv(file.path(dat.path, "temperature.csv"))

#calculate annual temps
d$temp <- rowMeans(d[, c(6:17)])

#get county data
CA <- shapefile(file.path(dat.path, "counties_2000.shp"))

#turn our temp data spatial for mapping and stuff
dsp <- SpatialPoints(d[,3:4], proj4string=CRS("+proj=longlat +datum=NAD83"))

#add in the dataframe d for calculations
dsp <- SpatialPointsDataFrame(dsp, d)
```

Spatial Transformations
```{r}
#teale albers CRS
TA <- CRS(" +proj=aea +lat_1=34 +lat_2=40.5 +lat_0=0 +lon_0=-120 +x_0=0 
          +y_0=-4000000 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")


#transform the SP objects dsp and CA to Teale Albers
dta <- spTransform(dsp, TA)
cata <- spTransform(CA, TA)
```

NULL
```{r}
#` here's the root mean squared error function
RMSE <- function(observed, predicted) {
  sqrt(mean((predicted - observed)^2, na.rm=TRUE))
}
```

```{r}
#get the temp mean
tavg <- mean(d$temp)

#get null RMSE
nullRMSE <- RMSE(tavg, dsp$temp)
nullRMSE
```

K-Fold
```{r}
#same randomness
set.seed(5162016)

#k-folds 5
k <- kfold(dta)

#now we have 5 groups randomly assigned
table(k)
```

Prepare to run a model(really we should be doin 5)
```{r}
#create test/train from one of the folds
test <- dta[k==1, ]
train <- dta[k!=1, ]

#get a df of the training data with x,y and temp
df <- data.frame(coordinates(train), temp=train$temp)
colnames(df)[1:2] = c('x', 'y')
```

```{r}
#run the glm on the training data
m <- glm(temp ~ x+y, data=df)
summary(m)
```

## Question 2

__Describe (in statistical terms) and explain (in physical terms) the results shown by summary(m)__

> The resulting table shows a coefficient for x that is positive but not statistically significant. It shows a negative coefficienct for y that is statistically significant. This means that as you move north, temperature goes down. 

## Question 3

__According to this model. How much does the temperature in California change if you travel 500 miles to the north (show the R code to compute that)__

```{r}
# 500 miles to meters (our model is in meters)
# multiply that by the coefficient for y 
# answer will be negative reflecting the decrease

#1 mile = 1609.344 meters
dist <- 500 * 1609.344

#get the coefficient for y and multiply by the distance
tchg <- coef(m)[3]*dist
```

> According to the model, the temperature will __decrease__ by about `r round(tchg, 2)*-1` degrees when travelling north by 500 miles.

****
5 Fold Cross Validation

```{r}
#get the testing data in a df for the loop
v <- data.frame(coordinates(test))
colnames(v)[1:2] = c('x', 'y')

#precition of the test data with the training model
p <- predict(m, v)
head(p)
```

```{r}
#set up a vector to fill
r <- rep(NA,5)

#loop 5 times
for (i in 1:5) {
  test <- dta[k==i, ] #set aside test data for each i
  train <- dta[k!=i, ] #set aside train data for each i
  df <- data.frame(coordinates(train), temp=train$temp) #create the train df
  m <- glm(temp ~ ., data=df) #run a model
  v <- data.frame(coordinates(test)) #get the test data for prediction
  p <- predict(m, v) #predict based on the training model
  r[i] <- RMSE(p, test$temp) #get the rmse for each i
}

#our five rmse's
r 

#the mean of the 5
mean(r)
```

## Question 4 

__Was it important to do 5-fold cross-validation, instead of a single 20-80 split?__

> The 5-fold cross-validation gives us a more accurate estimation of RMSE than a single 20-80 split.

****

Other ways of estimation

Raster the hard way?
```{r}
#create a raster grid of california
r <- raster(round(extent(cata)), res=10000, crs=TA)

# get the x coordinates
x <- init(r, v='x')
# set areas outside of CA to NA
x <- mask(x, cata)
# get the y coordinates
y <- init(r, v='y')
# combine the two variables (RasterLayers)
s <- stack(x,y)
names(s) <- c('x', 'y')

```

Make a model
```{r}
#df of the coords w/ temp
df <- data.frame(coordinates(dta), temp=dta$temp)
colnames(df)[1:2] = c('x', 'y')

#glm model
m <- glm(temp ~ ., data=df)
# predict
trend <- predict(s, m)

#get the aic of the model
linearAIC <- AIC(m)

#and the rmse
linearRMSE <- RMSE(predict(m), df$temp)


plot(trend, main = "Annual Temp in CA \n Basic Linear Model")
```

Can do the same thing with interpolate
```{r}

#here we are basically doing what we did above but with interpolate
z <- interpolate(r, m)

#making a mask to get the shape
mask <- mask(z, cata)

#getting the raster to plot
zm <- mask(z, mask)

plot(zm, main = "Annual Temp in CA \n Basic Linear Model")
```


w/ interaction
```{r}
#df with all data, xy+temp
df <- data.frame(coordinates(dta), temp=dta$temp)
colnames(df)[1:2] = c('x', 'y')

#set aside training and test from our previous k-fold `k`
test <- df[k==1, ]
train <- df[k!=1, ]

#interatction model
m <- glm(temp ~ x*y, data=train)
summary(m)

#get aic for comparison
intAIC <- AIC(m)

#get RMSE for comparison
intRMSE <- RMSE(predict(m, test), test$temp)

#interpolate 
z <- interpolate(r, m)

#mask
zm <- mask(z, mask)

#plot w/ contours
plot(zm, main = "Annual Temp in CA \n x*y Interaction Model")
contour(zm, add=TRUE)
```


with polynomial
```{r}
#run model with polynomial terms
#data = train, not df
m <- glm(temp ~ x + y + I(x^2) + I(y^2), data=train)
summary(m)

#get aic
polyAIC <- AIC(m)

#get RMSE
polyRMSE <- RMSE(predict(m, test), test$temp)

#interpolate and plot
z <- interpolate(r, m)
zm <- mask(z, mask)
plot(zm, main = "Annual Temp in CA \n Polynomial Model")
contour(zm, add=TRUE)
```

second order polynomials and interactions
```{r}
#run model with poly + interactions
#data = train, not df
m <- glm(temp ~ poly(x, 2, raw=TRUE) * poly(y, 2, raw=TRUE), data=train)
summary(m)

#get aic for comparing
poly2AIC <- AIC(m)

#get RMSE for comparing
poly2RMSE <- RMSE(predict(m, test), test$temp)

#interpolate and plot
z <- interpolate(r, m)
zm <- mask(z, mask)
plot(zm, main = "Annual Temp in CA \n Polynomial and Interaction Model")
contour(zm, add=TRUE)
```

## Question 5 

__What is the best model sofar? Why?__

```{r}
#put the AICs in a vector for plotting
AICs <- c(linearAIC, intAIC , polyAIC, poly2AIC)

#plot the AICs
par(mfrow=c(1,2))
plot(AICs, type="l", lwd=1.5, xaxt="n", xlab="Model Specification", main = "AIC Comparison")
axis(1, at=1:4,labels=F) #4= number of models
labels<-c("OLS", "Inter","Poly", "Poly + Int")
text(1:4, par("usr")[3]-.25, srt=45, adj=1.3, labels=labels, xpd=T)

#circle the model with the lowest AIC
symbols(x= which.min(AICs), y=AICs[which.min(AICs)], circles=.2, fg=2,lwd=2,add=T, inches = FALSE)

#put the RMSEs in a vector for plotting
RMSEs <- c(nullRMSE, linearRMSE , intRMSE, polyRMSE, poly2RMSE)

plot(RMSEs, type="l", lwd=1.5, xaxt="n", xlab="Model Specification", main = "RMSE Comparison")
axis(1, at=1:5,labels=F) #4= number of models
labels<-c("NULL", "OLS", "Inter","Poly", "Poly + Int")
text(1:5, par("usr")[3]-.25, srt=45, adj=.25, labels=labels, xpd=T)

#circle the model with the lowest RMSE
symbols(x= which.min(RMSEs), y=RMSEs[which.min(RMSEs)], circles=0.2, fg=2,lwd=2,add=T, inches = FALSE)
par(mfrow=c(1,1))
```

> Looking at the plot comparisons, the second order polynomial + interactions model (poly + int) has the lowest RMSE as well as the lowest AIC. Indicating it may have a balance balance between bias and variance. The RMSE indicates a model with high predictive power and the AIC indicates lower variance and more inferential power. NOTE: When we first ran these according to the lab guide, the polynomial models were incorrectly using the full data set, which gave VERY different results, this has been corrected for, which changed the AIC results quite a bit.

## Question 6

__Rerun the last model using (a) ridge regression, and (b) lasso regression. Show the changes in coefficients for three values of lambda; by finishing the code below__

### Ridge
```{r}
# this is our interaction model 
f <- temp ~ poly(x, 2, raw=TRUE) * poly(y, 2, raw=TRUE)

#make a matrix with the interactions in the dataframe
x <- model.matrix(f, df)

#ridge model: lasso ( alpha= 1, the default) and ridge (alpha = 0)
ridge <- glmnet(x, df$temp, alpha = 0, nlambda = 3)

#ridge coefficients
coef(ridge)
```

### Lasso
```{r}
#lasso model: lasso ( alpha= 1, the default) and ridge (alpha = 0)
lasso <- glmnet(x, df$temp, alpha = 1, lambda = c(0.24, 24, 2400))

#lasso coefficients
coef(lasso)
```

### Difference (changes)
```{r}
#the difference between the coefficients
coef(ridge) - coef(lasso)
```

****

Local regression
```{r}

#here we do a loess regression
m <- loess(temp ~ x + y, span=.2, data=train)

#interpolate and mask and plot
z <- interpolate(r, m)
zm <- mask(z, mask)
plot(zm, main = "Annual Temp in CA \n Local Model")
contour(zm, add=TRUE)

#get RMSE for comparing?
localRMSE <- RMSE(predict(m, test), test$temp)
```

## Question 7

__What does the the “span” argument represent?__

> the `span` argument represents the degree of smoothing. Too high of a span and the plot loses some of the detail. 

****

TPS
```{r}
#thin plate spline, x = ind variable (the coords), y = dependent (the temp)
tps <- Tps(train[, c('x', 'y')],  train$temp)
tps

#interpolate with TPS and plot
ptps <- interpolate(r, tps)
ptps <- mask(ptps, mask)
plot(ptps, main = "Annual Temp in CA \n TPS Model")
contour(ptps, add=TRUE)

#get pridictions of the test
pt <- predict(tps, test[, c('x', 'y')])
tpsRMSE <- RMSE(pt, test$temp)

```


Bring in Elevation as a covariate
```{r}
#get elevation for ca
elv1 <- raster::getData('worldclim', res=0.5, var='alt', lon=-122, lat= 37)
elv2 <- raster::getData('worldclim', res=0.5, var='alt', lon=-120, lat= 37)

#merge together in one raster
elv <- merge(elv1, elv2, overlap=FALSE)

#project and plot
telv <- projectRaster(elv, r)
celv <- mask(telv, mask)
names(celv) <- 'elevation'


```

Model Building
```{r}
#get test and train elv data
train$elevation <- extract(celv, train[, 1:2])
test$elevation <- extract(celv, test[, 1:2])

#remove NAs
train <- train[!is.na(train$elevation), ]
test <- test[!is.na(test$elevation), ]

#run another TPS with elevation included as a dependent var
tps2 <- Tps(train[, c('x', 'y', 'elevation')], train$temp)


# let's interpolate and plot
ptps2 <- interpolate(celv, tps2, xyOnly=FALSE)
plot(ptps2, main = "Annual Temp in CA \n TPS w/ Elevation Model")
contour(ptps2, add=TRUE)



#get predictions
pt2 <- predict(tps2, test[,  c('x', 'y', 'elevation')])

#get the RMSE 
elvRMSE <- RMSE(test$temp, pt2)
```



## Question 8

__What is a main reason that this the best prediction sofar?__

```{r}
#add in the new model's RMSE
RMSEs <-  c(RMSEs, localRMSE, tpsRMSE, elvRMSE)

plot(RMSEs, type="l", lwd=1.5, xaxt="n", xlab="Model Specification", main = "RMSE Comparison")
axis(1, at=1:8,labels=F) #4= number of models
labels<-c("NULL", "OLS", "Inter", "Poly", "Poly+Int", "Local", "Tps", "Tps+Elv")
text(1:8, par("usr")[3]-.25, srt=45, adj=.25, labels=labels, xpd=T)

#circle the model with the lowest RMSE
symbols(x= which.min(RMSEs), y=RMSEs[which.min(RMSEs)], circles=0.2, fg=2,lwd=2,add=T, inches = FALSE)
par(mfrow=c(1,1))

```

> With the lowest RMSE, the TPS with Elevation is the best model so far. I think this is because bringing in another relevant covariate such as elevation increases the accuracy of the predictions, because more the you know to start, the better off you are in the end. 

****

GAM Models

```{r}
#run the Generalized Additive Model
#note the terms are in s(), this is a smoothing function for GAM models
ga <- gam(temp ~ s(x) + s(y) + s(elevation), data=train)

#interpolate and plot
x <- interpolate(celv, ga, xyOnly=FALSE)
plot(x)
contour(x, add=TRUE)

#make predictions
pg <- predict(ga, test)

#get RMSE
RMSE(test$temp, pg)
```

Individual Plots for Variables
```{r}
#This gets the plots for each variable
plot.gam(ga, pages=1)
```

## Question 9 

__Use the help files to exaplin the model below. What do you think of it? Is it better or worse than the gam we did above?__

```{r}
#` bs is the smothing penalty basis, ts is thin plate spline with null space, can shrink terms to 0
#` te() = tensor product smoothing, k = 12 raises the number of degrees of freedom possible
#` gam needs smoothed terms so these functions specify the type of smoothing
ga2 <- gam(temp ~ te(x, y, k=12, bs='ts') + s(elevation, bs='ts'), data=train)

#get predictions
pg <- predict(ga2, test)

#get RMSE
RMSE(test$temp, pg)
```

Get individual plots
```{r}
plot.gam(ga2, pages=1)
```

> The RMSE for this is the lowest of all the models in the lab. I think this indicates some level of improved predictions. The gam plots for x, y looks similar to California in terms of space, though I'm not totally sure what it means. I put some comments above the model to indicate what is going on.